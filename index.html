<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="ICG-Net: A Unified Approach for Instance-Centric Grasping">
  <meta name="keywords" content="Grasping, Manipulation, Implicit-Representations">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ICG-Net: A Unified Approach for Instance-Centric Grasping</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ICG-Net: A Unified Approach for Instance-Centric Grasping</h1>
            <h3 class="title is-4">ICRA 2024</h3>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://renezurbruegg.github.io/">René Zurbrügg</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.ch/citations?user=ksQ4JnQAAAAJ&hl=ko">Yifan Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://francisengelmann.github.io/">Francis Engelmann</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://suryanshkumar.github.io/">Suryansh Kumar</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://rsl.ethz.ch/the-lab/people/person-detail.MTIxOTEx.TGlzdC8yNDQxLC0xNDI1MTk1NzM1.html">Marco Hutter</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=aB04078AAAAJ&hl=en">Vaishakh Patil</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.yf.io/">Fisher Yu</a><sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ETH Zürich,</span>
              <span class="author-block"><sup>2</sup>Texas A&M University</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.09939" class="external-link button is-normal is-rounded isdark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.09939" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/renezurbruegg/icg_net" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/renezurbruegg/icg_benchmark"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Benchmark</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/realworld_sm.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <span>ICG-Net</span> directly predicts 6-DoF grasps, segmentation masks and mesh reconstructions
          for each instance in a cluttered scenes.
        </h2>
      </div>
    </div>
  </section>


  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centred">
          <div class="column">

            <div class="content has-text-centered">
              <h2 class="">
                Instance Centric Predictions for Grasping and Scene Understanding
                </h4>
            </div>
          </div>
        </div>
        <div class="columns is-centred">
          <div class="column">

            <div class="content has-text-centered">
              <h4 class="title is-4">Input Pointcloud</h4>
              <video poster="" id="scan" autoplay controls muted loop playsinline width="40%">
                <source src="./static/videos/pc_cropped_scan.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <div class="columns is-centred">
          <div class="column is-4">
            <div class="content has-text-centered">
              <h4 class="title is-4">Segmentation</h4>
              <video poster="" id="seg" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/pc_cropped_seg.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="column is-4">
            <div class="content has-text-centered">
              <h4 class="title is-4">Grasp Prediction</h4>
              <video poster="" id="grasp" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/grasps_cropped.mp4" type="video/mp4">
            </div>
          </div>
          <div class="column is-4">
            <div class="content has-text-centered">
              <h4 class="title is-4">Reconstruction</h4>
              <video poster="" id="recon" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/mesh_cropped.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        <div id="results-carousel" class="carousel results-carousel">
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Accurate grasping is the key to several robotic tasks including assembly and household robotics.
              Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding:
              <br>
              First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps.
              These grasps need to be compliant with the local object geometry. </br>
              Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in
              the scene.</br>
              Finally, the robot must compute a collision-free grasp trajectory while taking into account the geometry
              of the target object.</br>
              Most grasp detection algorithms directly predict grasp poses in a monolithic fashion, which does not
              capture the composability of the environment.
            </p>
            <p>
              In this paper, we introduce an end-to-end architecture for object-centric grasping.
              The method uses pointcloud data from a single arbitrary viewing direction as an input and generates an
              instance-centric representation for each partially observed object in the scene.
              This representation is further used for object reconstruction and grasp detection in cluttered table-top
              scenes.
            </p>
            <p>
              We show the effectiveness of the proposed method by extensively evaluating it against state-of-the-art
              methods on synthetic datasets, indicating superior performance for grasping and reconstruction.
              Additionally, we demonstrate real-world applicability by decluttering scenes with varying numbers of
              objects.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">

          <div class="content has-text-centered">
            <h2 class="title is-3">Model Architecture</h2>
            <div class="method">
              <img src="./static/images/method.svg" alt="ICG-Net" style="width:120%" />
              <div class="content has-text-justified">
                <p>
                  Given an input pointcloud, we voxelize the pointcloud and extract volumetric and surface features at
                  multiple scales using a sparse Minkowski and dense U-Net.
                  The surface features are enriched with volumetric information and treated as tokens with positional
                  encodings based on voxel locations.
                </p>
                <p>
                  Masked attention iteratively refines instance queries by cross-attending to extracted sparse tokens.
                  This process allows each latent query to focus on a specific instance and to be classified as
                  <span>"semantic class"</span> or <span>"no object"</span>
                  The refined queries condition the task-specific decoders to model the occupancy of each instance
                  directly or to predict grasp affordance scores and gripper widths.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>

  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-12">
          <h2 class="title">Simulation Examples</h2>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-6">
          <h6 class="title is-3">Packed Scene</h6>
          <h5 class="">Input</h5>
          <video poster="" id="packed" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/pc_packed.mp4" type="video/mp4">
        </div>
        <div class="column is-6">
          <h6 class="title is-3">Pile Scene</h6>
          <h5 class="">Input</h5>
          <video poster="" id="piled" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/pc_piled.mp4" type="video/mp4">
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-6">
          <h3 class="">Prediction</h3>
          <video poster="" id="pred_packed" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/pred_packed.mp4" type="video/mp4">
        </div>
        <div class="column is-6">
          <h5 class="">Prediction</h5>
          <video poster="" id="pred_piled" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/pred_piled.mp4" type="video/mp4">
        </div>
      </div>
    </div>

  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{zurbrugg2024icgnet,
  title={ICGNet: A Unified Approach for Instance-Centric Grasping},
  author={Zurbr{\"u}gg, Ren{\'e} and Liu, Yifan and Engelmann, Francis and Kumar, Suryansh and Hutter, Marco and Patil, Vaishakh and Yu, Fisher},
  journal={arXiv preprint arXiv:2401.09939},
  year={2024}
}
</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>